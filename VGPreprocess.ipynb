{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook preprocesses the VG dataset as was described in the supplementary of our paper. Please first make sure you have downloaded the datasset from https://visualgenome.org/api/v0/api_home.html at the latest revision.\n",
    "\n",
    "Please also install the python driver from https://github.com/ranjaykrishna/visual_genome_python_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visual_genome.local as vg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image as PIL_Image\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/ssd/tobias/datasets/vg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = json.load(open(data_dir + \"attributes.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = json.load(open(data_dir + \"objects.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 1, 'objects': [{'synsets': ['tree.n.01'], 'h': 557, 'object_id': 1058549, 'merged_object_ids': [], 'names': ['trees'], 'w': 799, 'y': 0, 'x': 0}, {'synsets': ['sidewalk.n.01'], 'h': 290, 'object_id': 1058534, 'merged_object_ids': [5046], 'names': ['sidewalk'], 'w': 722, 'y': 308, 'x': 78}, {'synsets': ['building.n.01'], 'h': 538, 'object_id': 1058508, 'merged_object_ids': [], 'names': ['building'], 'w': 222, 'y': 0, 'x': 1}, {'synsets': ['street.n.01'], 'h': 258, 'object_id': 1058539, 'merged_object_ids': [3798578], 'names': ['street'], 'w': 359, 'y': 283, 'x': 439}, {'synsets': ['wall.n.01'], 'h': 535, 'object_id': 1058543, 'merged_object_ids': [], 'names': ['wall'], 'w': 135, 'y': 1, 'x': 0}, {'synsets': ['tree.n.01'], 'h': 360, 'object_id': 1058545, 'merged_object_ids': [], 'names': ['tree'], 'w': 476, 'y': 0, 'x': 178}, {'synsets': ['shade.n.01'], 'h': 189, 'object_id': 5045, 'merged_object_ids': [], 'names': ['shade'], 'w': 274, 'y': 344, 'x': 116}, {'synsets': ['van.n.05'], 'h': 176, 'object_id': 1058542, 'merged_object_ids': [1058536], 'names': ['van'], 'w': 241, 'y': 278, 'x': 533}, {'synsets': ['trunk.n.01'], 'h': 348, 'object_id': 5055, 'merged_object_ids': [], 'names': ['tree trunk'], 'w': 78, 'y': 213, 'x': 623}, {'synsets': ['clock.n.01'], 'h': 363, 'object_id': 1058498, 'merged_object_ids': [], 'names': ['clock'], 'w': 77, 'y': 63, 'x': 422}, {'synsets': ['window.n.01'], 'h': 147, 'object_id': 3798579, 'merged_object_ids': [], 'names': ['windows'], 'w': 198, 'y': 1, 'x': 602}, {'synsets': ['man.n.01'], 'h': 248, 'object_id': 3798576, 'merged_object_ids': [1058540], 'names': ['man'], 'w': 82, 'y': 264, 'x': 367}, {'synsets': ['man.n.01'], 'h': 259, 'object_id': 3798577, 'merged_object_ids': [], 'names': ['man'], 'w': 57, 'y': 254, 'x': 238}, {'synsets': [], 'h': 430, 'object_id': 1058548, 'merged_object_ids': [], 'names': ['lamp post'], 'w': 43, 'y': 63, 'x': 537}, {'synsets': ['sign.n.02'], 'h': 179, 'object_id': 1058507, 'merged_object_ids': [], 'names': ['sign'], 'w': 78, 'y': 13, 'x': 123}, {'synsets': ['car.n.01'], 'h': 164, 'object_id': 1058515, 'merged_object_ids': [], 'names': ['car'], 'w': 80, 'y': 342, 'x': 719}, {'synsets': ['back.n.01'], 'h': 164, 'object_id': 5060, 'merged_object_ids': [], 'names': ['back'], 'w': 70, 'y': 345, 'x': 716}, {'synsets': ['jacket.n.01'], 'h': 98, 'object_id': 1058530, 'merged_object_ids': [], 'names': ['jacket'], 'w': 82, 'y': 296, 'x': 367}, {'synsets': ['car.n.01'], 'h': 95, 'object_id': 5049, 'merged_object_ids': [], 'names': ['car'], 'w': 78, 'y': 319, 'x': 478}, {'synsets': ['trouser.n.01'], 'h': 128, 'object_id': 1058531, 'merged_object_ids': [], 'names': ['pants'], 'w': 48, 'y': 369, 'x': 388}, {'synsets': ['shirt.n.01'], 'h': 103, 'object_id': 1058511, 'merged_object_ids': [], 'names': ['shirt'], 'w': 54, 'y': 287, 'x': 241}, {'synsets': ['parking_meter.n.01'], 'h': 143, 'object_id': 1058519, 'merged_object_ids': [], 'names': ['parking meter'], 'w': 26, 'y': 325, 'x': 577}, {'synsets': ['trouser.n.01'], 'h': 118, 'object_id': 1058528, 'merged_object_ids': [], 'names': ['pants'], 'w': 44, 'y': 384, 'x': 245}, {'synsets': ['shirt.n.01'], 'h': 102, 'object_id': 1058547, 'merged_object_ids': [], 'names': ['shirt'], 'w': 82, 'y': 295, 'x': 368}, {'synsets': ['shoe.n.01'], 'h': 28, 'object_id': 1058525, 'merged_object_ids': [5048], 'names': ['shoes'], 'w': 48, 'y': 485, 'x': 388}, {'synsets': ['arm.n.01'], 'h': 41, 'object_id': 1058546, 'merged_object_ids': [], 'names': ['arm'], 'w': 30, 'y': 285, 'x': 370}, {'synsets': ['bicycle.n.01'], 'h': 36, 'object_id': 1058535, 'merged_object_ids': [], 'names': ['bike'], 'w': 27, 'y': 319, 'x': 337}, {'synsets': ['bicycle.n.01'], 'h': 41, 'object_id': 5051, 'merged_object_ids': [], 'names': ['bike'], 'w': 27, 'y': 311, 'x': 321}, {'synsets': ['headlight.n.01'], 'h': 9, 'object_id': 5050, 'merged_object_ids': [], 'names': ['headlight'], 'w': 18, 'y': 370, 'x': 517}, {'synsets': ['spectacles.n.01'], 'h': 23, 'object_id': 1058518, 'merged_object_ids': [], 'names': ['glasses'], 'w': 43, 'y': 317, 'x': 448}, {'synsets': ['chin.n.01'], 'h': 8, 'object_id': 1058541, 'merged_object_ids': [], 'names': ['chin'], 'w': 9, 'y': 288, 'x': 401}], 'image_url': 'https://cs.stanford.edu/people/rak248/VG_100K_2/1.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# An example of the format of the annotations.\n",
    "print(objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(objects[0][\"objects\"]))\n",
    "print(len(attributes[0][\"attributes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046  not in objects file.\n",
      "1058529  not in objects file.\n",
      "5048  not in objects file.\n",
      "1058532  not in objects file.\n",
      "1058536  not in objects file.\n",
      "3798575  not in objects file.\n",
      "1058540  not in objects file.\n",
      "1058544  not in objects file.\n",
      "3798578  not in objects file.\n"
     ]
    }
   ],
   "source": [
    "idlist = [obj[\"object_id\"] for obj in objects[0][\"objects\"]]\n",
    "seen_list = []\n",
    "for obj in attributes[0][\"attributes\"]:\n",
    "    if obj[\"object_id\"] not in idlist:\n",
    "        print(obj[\"object_id\"], \" not in objects file.\")\n",
    "    elif obj[\"object_id\"] in seen_list:\n",
    "        print(obj[\"object_id\"], \" seen twice.\")\n",
    "    else:\n",
    "        seen_list.append(obj[\"object_id\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that maps the words to the corresponding synsets\n",
    "mysynsets = json.load(open(data_dir + \"attribute_synsets.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse.s.01\n"
     ]
    }
   ],
   "source": [
    "print(mysynsets[\"sparse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_vg_dicts(vg_dir, val=False):\n",
    "    \"\"\" Return the detectron-style dict for the VG dataset. See\n",
    "        https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html\n",
    "        The last 10k images are used as val, if val=True.\n",
    "    \"\"\"\n",
    "    im_list = vg.get_all_image_data(data_dir=vg_dir) # image metadata\n",
    "    print(len(im_list))\n",
    "    \n",
    "    if val == False:\n",
    "        appearing_objects = {} # maps synset_id -> int\n",
    "        appearing_attributes = {} # maps synset_id -> int\n",
    "    else:\n",
    "        appearing_objects = json.load(open(vg_dir + \"appearing_objects.json\"))\n",
    "        appearing_attributes = json.load(open(vg_dir + \"appearing_attributes.json\"))\n",
    "        \n",
    "    mysynsets = json.load(open(vg_dir + \"attribute_synsets.json\"))\n",
    "    #objdata = json.load(open(vg_dir + \"objects.json\"))\n",
    "    attributes = json.load(open(vg_dir + \"attributes.json\"))\n",
    "    # Preprocess objects (make list searchable by id)\n",
    "    #corresponding_objects = {objlist[\"image_id\"] : objlist[\"objects\"] for objlist in objdata}\n",
    "    corresponding_obj_wattributes = {attobj[\"image_id\"] : attobj[\"attributes\"] for attobj in attributes}\n",
    "    striplen = len(\"https://cs.stanford.edu/people/rak248/\") # remove from image url                 \n",
    "    print(f\"Preprocessing done.\")\n",
    "    # Train, val split\n",
    "    if val:\n",
    "        lbegin, lend = 98077, 108077\n",
    "    else:\n",
    "        lbegin, lend = 0, 98077\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    curr_id = 0\n",
    "    curr_att_id = 0\n",
    "    for idx, image_meta in tqdm(enumerate(im_list[lbegin:lend])):\n",
    "        record = {}\n",
    "        #graph = vg.get_scene_graph(image_meta.id, images=vg_dir, image_data_dir=vg_dir + 'by-id/', synset_file= vg_dir + 'synsets.json')\n",
    "        filename = os.path.join(vg_dir, image_meta.url[striplen:])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = image_meta.id\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        shown_objects = corresponding_obj_wattributes[image_meta.id]\n",
    "        objs = []\n",
    "        for spec_object in shown_objects:\n",
    "            for synsetname in spec_object[\"synsets\"]:\n",
    "                if synsetname not in appearing_objects.keys():\n",
    "                    if val:\n",
    "                        continue\n",
    "                    appearing_objects[synsetname] = curr_id # add\n",
    "                    curr_id += 1\n",
    "                my_attlist = [] # list attribute ids here.\n",
    "                if \"attributes\" in spec_object:\n",
    "                    for att in spec_object[\"attributes\"]:\n",
    "                        attmod = att.lower().strip()\n",
    "                        if attmod in mysynsets.keys():\n",
    "                            att_id = mysynsets[attmod] # find the sysnset att_id\n",
    "                        else:\n",
    "                            #print(f\"Attribute: {attmod} not found in synsets dict\")\n",
    "                            continue\n",
    "\n",
    "                        if att_id not in appearing_attributes.keys():\n",
    "                            if val:\n",
    "                                continue\n",
    "                            appearing_attributes[att_id] = curr_att_id # add\n",
    "                            curr_att_id += 1\n",
    "                        my_attlist.append(appearing_attributes[att_id])\n",
    "                obj = {\n",
    "                    \"bbox\": [spec_object[\"x\"], spec_object[\"y\"], spec_object[\"w\"], spec_object[\"h\"]],\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": appearing_objects[synsetname],\n",
    "                    \"attribute_ids\": my_attlist\n",
    "                }\n",
    "                objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts, appearing_objects, appearing_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108077\n",
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98077it [04:58, 328.11it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, appearing_objects, appearing_attributes = get_vg_dicts(data_dir, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7842\n",
      "Number of attributes: 6277\n"
     ]
    }
   ],
   "source": [
    "fname = data_dir + \"detectron_train.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(dataset, fhandle)\n",
    "fhandle.close()\n",
    "\n",
    "print(f\"Number of classes: {len(appearing_objects.keys())}\")\n",
    "fname = data_dir + \"appearing_objects.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(appearing_objects, fhandle)\n",
    "fhandle.close()\n",
    "\n",
    "print(f\"Number of attributes: {len(appearing_attributes.keys())}\")\n",
    "fname = data_dir + \"appearing_attributes.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(appearing_attributes, fhandle)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108077\n",
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:26, 374.04it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, _, _ = get_vg_dicts(data_dir, val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = data_dir + \"detectron_val.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(dataset, fhandle)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = data_dir + \"appearing_objects.json\"\n",
    "appearing_objects = json.load(open(fname))\n",
    "fname = data_dir + \"appearing_attributes.json\"\n",
    "appearing_attributes = json.load(open(fname))\n",
    "fname = data_dir + \"detectron_train.json\"\n",
    "dataset = json.load(open(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from image_id -> dataset index.\n",
    "id_to_idx = {inst[\"image_id\"]: i for i, inst in enumerate(dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35938"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_idx[2385321]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now filter all objects with less than 8 occurances.\n",
    "Because of the variety of things we filter out classes and attributes that appear so seldomly that the classifier has no chance to learn them. We didn't want to reduce the coverage too much, but classes below 8 items were almost never predicted anyway.\n",
    "Technical remark ``appearing_objects``, ``appearing_attributes`` should still be defined from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vg_dicts_cached(vg_dir, val=False):\n",
    "    \"\"\" Read the dataset dicts. \"\"\"\n",
    "    fname = vg_dir + \"detectron_\" + (\"val\" if val else \"train\") + \".json\"\n",
    "    fhandle = open(fname)\n",
    "    data = json.load(fhandle)\n",
    "    fhandle.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_vg_dicts_cached(data_dir, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_appearing_objects = {v: k for k,v in appearing_objects.items()} # Reverse dict -> synset \n",
    "rev_appearing_attributes = {v: k for k,v in appearing_attributes.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First object   ['clock.n.01', 'street.n.01', 'shade.n.01', 'man.n.01', 'gym_shoe.n.01', 'headlight.n.01', 'car.n.01', 'bicycle.n.01', 'sign.n.02', 'building.n.01', 'trunk.n.01', 'sidewalk.n.01', 'shirt.n.01', 'back.n.01', 'spectacles.n.01', 'parking_meter.n.01', 'shoe.n.01', 'trouser.n.01', 'jacket.n.01', 'chin.n.01', 'guy.n.01', 'van.n.05', 'wall.n.01', 'tree.n.01', 'arm.n.01']\n",
      "First objects occurance counts: [9394, 10717, 1723, 73410, 3574, 4802, 21942, 8489, 33450, 35032, 6918, 8950, 38316, 2993, 6043, 530, 16003, 14051, 11484, 564, 2800, 2172, 34511, 54270, 10094]\n"
     ]
    }
   ],
   "source": [
    "occurance_count = {} # synset -> num_occurances(int)\n",
    "occurance_attr = {} # synset -> num_occurances(int)\n",
    "for item in dataset:\n",
    "    for obj in item[\"annotations\"]:\n",
    "        if  obj[\"category_id\"] in occurance_count:\n",
    "            occurance_count[obj[\"category_id\"]] += 1\n",
    "        else:\n",
    "            occurance_count[obj[\"category_id\"]] = 1\n",
    "        for att in obj[\"attribute_ids\"]:\n",
    "            if  att in occurance_attr:\n",
    "                occurance_attr[att] += 1\n",
    "            else:\n",
    "                occurance_attr[att] = 1\n",
    "                \n",
    "print(\"First object  \", [rev_appearing_objects[k] for k in range(25)])               \n",
    "print(\"First objects occurance counts:\", list(occurance_count.values())[:25])\n",
    "\n",
    "#print(list(occurance_attr.values())[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining classes: 3434\n",
      "Remaining attributes: 2979\n"
     ]
    }
   ],
   "source": [
    "classes_remain = 0\n",
    "attributes_remain = 0\n",
    "n = 8\n",
    "for k,v in occurance_count.items():\n",
    "    if v > n:\n",
    "        classes_remain += 1\n",
    "for k,v in occurance_attr.items():\n",
    "    if v > n:\n",
    "        attributes_remain += 1\n",
    "print(f\"Remaining classes: {classes_remain}\")\n",
    "print(f\"Remaining attributes: {attributes_remain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping that maps the object_ids to their new ids and the attribute_ids to their new ids\n",
    "id_to_set = {v: k for k, v in appearing_objects.items()}\n",
    "att_id_to_set = {v: k for k, v in appearing_attributes.items()}\n",
    "new_ids = {}\n",
    "idx = 0\n",
    "for k,v in occurance_count.items():\n",
    "    if v > n:\n",
    "        new_ids[k] = idx\n",
    "        idx +=1\n",
    "\n",
    "idx = 0\n",
    "new_ids_attr = {}\n",
    "for k,v in occurance_attr.items():\n",
    "    if v > n:\n",
    "        new_ids_attr[k] = idx\n",
    "        idx +=1\n",
    "        \n",
    "new_to_old = {v: k for k, v in new_ids.items()}\n",
    "new_to_old_att = {v: k for k, v in new_ids_attr.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the item-ids to the filtered version. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old ID: 1934\n",
      "New ID: 1735\n"
     ]
    }
   ],
   "source": [
    "print(\"Old ID:\", appearing_attributes[\"neck.n.01\"])\n",
    "print(\"New ID:\", new_ids_attr[appearing_attributes[\"neck.n.01\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_index(dataset):\n",
    "    \"\"\" Update the dataset to the new ids of attributes and objects. Remove objects and attributes that do not apprear often enough.\n",
    "        Additionally sort the attributes to have the most common attribute first.\n",
    "    \"\"\"\n",
    "    for item in dataset:\n",
    "        myobjects = item[\"annotations\"]\n",
    "        newobjects = []\n",
    "        for obj in myobjects:\n",
    "            if  obj[\"category_id\"] in new_ids:\n",
    "                obj[\"category_id\"] = new_ids[obj[\"category_id\"]]\n",
    "                newobjects.append(obj)\n",
    "            new_attrs = []\n",
    "            for myattribute in obj[\"attribute_ids\"]:\n",
    "                if myattribute in new_ids_attr.keys(): # It has made the cut\n",
    "                    my_count = occurance_attr[myattribute] # occurances of own attribute\n",
    "                    # Sort the most common attribute first.\n",
    "                    for idx, elem in enumerate(new_attrs):\n",
    "                        if occurance_attr[new_to_old_att[elem]] < my_count:\n",
    "                            new_attrs.insert(idx, new_ids_attr[myattribute])\n",
    "                            break\n",
    "                    if myattribute not in new_attrs: # Occurance is smaller than all previous\n",
    "                        new_attrs.append(new_ids_attr[myattribute])\n",
    "            obj[\"attribute_ids\"] = new_attrs\n",
    "        item[\"annotations\"] = newobjects\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = update_index(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = data_dir + \"detectron_train_filtered.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(dataset, fhandle)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new class labels.\n",
    "classesdict = {nid: id_to_set[oldid] for nid, oldid in new_to_old.items()}\n",
    "attrdict = {nid: att_id_to_set[oldid] for nid, oldid in new_to_old_att.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = data_dir + \"appearing_objects_filtered.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(classesdict, fhandle)\n",
    "fhandle.close()\n",
    "\n",
    "fname = data_dir + \"appearing_attributes_filtered.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(attrdict, fhandle)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_vg_dicts_cached(data_dir, val=True)\n",
    "\n",
    "dataset = update_index(dataset)\n",
    "\n",
    "fname = data_dir + \"detectron_val_filtered.json\"\n",
    "fhandle = open(fname, \"w\")\n",
    "json.dump(dataset, fhandle)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: Read files and print max ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3433 2978\n"
     ]
    }
   ],
   "source": [
    "fname = data_dir + \"detectron_train_filtered.json\"\n",
    "data = json.load(open(fname, \"r\"))\n",
    "maxid = 0\n",
    "maxattid = 0\n",
    "for item in data:\n",
    "    for objectann in item[\"annotations\"]:\n",
    "        maxid = max(maxid, objectann[\"category_id\"])\n",
    "        maxattid = max(maxattid, objectann[\"attribute_ids\"][0] if len(objectann[\"attribute_ids\"]) else 0)\n",
    "print(maxid, maxattid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
